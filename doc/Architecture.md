# The Architecture of BaudEngine

BaudEngine is distributed database engine with elastic storage and flexible search.

## Data Model

Field, Object, Space, Edge, DB

* Object (Document)

OID (primary key) -> fields

OID can be specified by applications or generated by the system

* Field

Field: key -> a scalar value, an array of values, or a nested structure

Any field can be indexed and morever full-text search is a first-class citizen. 

Formally, a field is defined as (name, value, property) where property = (dataType, indexType, analyzer).

* Field Datatypes

string, numberic, date, boolen, binary

array, object

Geo-point, geo-shape

* Analysis

text -> term

* Routing

hash of the routing key

document -> partition

* Mapping

Each space has a 'mapping' to specify the field types, how to index, analyzer etc.

Usually a scalar field is defined as the primary key and a scalar field as the routing key. And they can be same.

* Dynamic Mapping

The mappings are not only defined explicitly, but can be generated automatically, i.e. you can add new fields on the insert time.

* Graph

Native support for the SPO model

Edges as a special kind of fields: predicates as keys, and an array of "space/oid" as values


## System Concepts

BaudEngine is a multi-datacenter distributed system. However, it can be run in the single-datacenter mode.  

### zone

Zones, or called cells, are the unit of physical isolation as well as admin deployment. 

### components

* Master

the global master of the entire cluster

the zone master per zone

both are based on raft replication for high availability

* Partition Server

the writer role, i.e. Writer Partition Server (WPS)

the reader role, i.e. Reader Partition Server (RPS)

* Client

the Go SDK directly talking with masters and PSes.

* Gateway Servers

BaudSQL - the NewSQL gateway with MySQL-compatible protocol

Router - the NoSQL gateway with ElasticSearch-compatible protocol

### software stack

* cluster management

conainer-native - BaudEngine can be run in Kubernetes, or on bare metal. 

each zone has a DCOS/Kubernetes interface to allocate PS nodes and router nodes. 

each DB is allocated with its own set of PS across different zones.

* BaudStorage

As the shared datacenter storage, BS is mounted to store partition data, which is log-structured sorted key-value files and redo-logs (WALs). 

Note BaudEngine can also run on local filesystems - actually BS is transparent to BE. 

### partitioning 

db -> space -> partition = partition key hash range

The partition count is planned according to the maximum write throughput that the space should provision. 

### replication

* cross-zone replication

Each partition has a Raft state machine located in three 'writer-role' partitionservers of different zones. Actually the writer-role partitionservers run the 'multi-raft' protocol. 

* within-zone replication

In any zone, a partition can have any number of read-only replicas, which are served by the 'Reader-Role' partitionservers reading the underlying Baudstorage files. 

### scalability

* goal

One Baud cluster can host one to thousands of databases; 
one DB can host billions of spaces;
one space can host unlimited number of objects;

* scaling up/down - resizing partitions

Moving partitions between PS nodes for load balancing is easy by leveraging the BaudStorage DCFS: stop a partition replica and restart from another partitionserver.

* scaling out/in - online re-sharding

Partition splitting & merging is implemented via filtered replication. 

However, we recommand that a space is pre-sharded and somehow over-sharded to avoid re-sharding on runtime.


### local index vs. global index

currently local index only. 


## Master

There are two tiers for the cluster topo metadata managment. 

### globalmaster

* core data structures

db and space schemas

zones

partitions -> zones

note that the globalmaster has no knowledge of nodes, neither PS nodes nor router nodes

* key operations

create DB and Spaces

split or merge a partition

### zonemaster

* core data structures

partitions -> PS nodes

* key operations

assign partitions to PS nodes

* failure detector (FD)

distributed voting of PS health

* placement driver (PD)


### implementation

based on a distributed coordination service like etcd.


## PartitionServer

there are two PS roles, i.e. two modes of running instances: 

* the writer role. participating multi-raft, a raft statemachine per partition, and logging the raft operations locally 

* the reader role. just reading from BaudStorage to serve partition read-only requests

### partition data store

Each partition has a single key-value storage engine for both objects and indexes, which is persisted onto the Baudstorage datacenter filesystem

* the primary index

(#PI, primaryKey, fieldName, fieldValue) -> timestamp

* secondary indexes

(#SI, fieldName, fieldValue or term, primaryKey) -> timestamp

### analyzer

term synonyms are stored as a file of Baudstorage and loaded by PS for document analysis.

### Key Operations


## Client

* PartitionClient

* ZoneMasterClient

* GlobalMasterClient


## Router

A zone has a group of router nodes as the service gateway for the application of the same zone. Note a router needs to interact with not only the zonemaster of its own zone but also the zonemasters of other zones. 

### Query Language

Router extends elasticsearch DSL: 

* Graph

## BaudSQL 

Tables sharing the same partition key = one space


## Deployment Flexibility

* single zone or multiple zones

* the BaudStorage datacenter filesystem or local filesystems

* Kubernetes or bare metal


## Backup and Restore

provide a point-in-time snapshot of the data on a partition


## Manageability

Ops Center

Dashboard

### Monitoring

cluster-level statistics

space-level info

individual nodes

GC

SlowLog

### Upgrade


## Applications

products

email

messaging

blogging


