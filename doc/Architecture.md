# Architecture

BaudEngine (Baud for short) is a distributed data serving system for elastic storage & intelligent search.

## Data Model

Field, Entity, Concept, Edge, Space, DB

Each entity has an internal 'Unique ID' (UID) generated by the system, which is unique aross the entire system. 

Entities as documents: UID -> Fields

Entity -instanceOf-> Concept -subclassOf-> Concept ...

Edges as documents: (UIDi, UIDj) -> Fields

Field: name -> a typed value or a sorted array of values

Any field of entities or edges can be indexed and morever full-text search is a first-class citizen. 


## Overview

### components

master, raft replication for high availability

partitionserver (ps)

router


### cluster management

conainer-native - baud runs on Kubernetes clusters

### partitioning & replication

db -> entity or edge space -> partition -> slot

partition = slot id range

3 or more partitionservers form a replication group by means of raft. 

multiple partitions of different spaces of the same db could be co-located on the same ps repl group.

### re-sharding

CFS is used for partition snapshot and log backup. 

And we also leverage it to do partition re-sharding. 

### scalability guarantee

One Baud cluster can host one to thousands of databases; 
one DB can host one to millions of spaces;
one space can host one to billions of objects;


## Master

three/five/.. BM instances form a replicated BM service, or leverage a distributed coordination service like etcd/consul to store the metadata of Baud itself. 

we currently choose the former approach. 

* e.g. Start a master via cmd shell,

host2:$ baud -cm -http-addr host2:5001 -raft-addr host2:5002 -topo http://host1:5001 -data ~/node


### data structures

* database metadata

db (name -> id)

space (name -> id): entity or edge

partition (slot id range of (source) entity uid) : entity or edge

* cluster topo metadata

master nodes

ps nodes

router nodes

### persistence

marshalled and written to boltdb

### key operations

* Create a Space

foreach partition among the space, select a destination PS raft group (PSRG) to create the partition.

* Split a Partition

1, select two PSRG as the destinations

2, notify the source partition leader to do streaming backup of WALs

2, call the two dest partition leaders to load the snapshot of the source partition and then consume the WAL stream from CFS

3, stop the source partition raft group

4, cutover

* Merge Partitions

the reverse process of split


* PS metrics reporting

* Router metrics reporting


## PS

Several PS nodes form a raft group, partitionserver group (PSG). And one PSG usualy serves a partition - a part of entity or edge space. 

### Kernel - inside a partition

The storage and indexing backend for each partition consists of several B-Tree tables. 

* FieldTable

for entity partition, (UID, fieldID) -> a single or multiple packed values of the field;

for edge partition, (<UID1, UID2>, fieldID) -> ...

* PostingListTable

foreach field, an in-memory B-Tree as the Dictionary, and the leaf values are the pointers to PostingLists. 

one optional implementation is as below: 

type PostingList struct {
    chunks []*PostingChunk
    numOfChunks int
    numOfPostings int
}

type PostingChunk struct {
    postingArray []byte
    capx int
    size int
}

* PositionListTable

type PositionList struct {
    positions []uint32
    wdf int
}

* SynonymTable

* TermListTable

UID -> TermList


### Key Operations


## Router

## Search

### Ranking

## Manageability

Ops Center

Dashboard

### Monitoring

cluster-level statistics

space-level info

individual nodes

GC

SlowLog

### Deployment and Configration


### Upgrade


## Applications

### document databases

### search engine

### social backend

email

messaging

blogging

### object storage metadata

buckets as spaces

object URL is indexed properly

### CFS metadata

a filesystem namespace as a baud space

an inode as an object with link (parentIno + "-" + name) as an indexing field (array)


